<center><h1 style="font-size: 24px;">爬虫基础</h1></center>

[TOC]



## 在开始之前

在开始正式学习之前，我们先讲一下爬虫是什么以及会涉及到的前端、网络基础知识。

### 什么是爬虫

简单来讲，它的基本操作就是模拟人的行为去各个网站溜达，点点按钮，查查数据，把看到的信息返还回来。

例如我们熟悉的**百度一下**就是应用了这种爬虫技术，每天发放出无数个爬虫到各个网站，把他们的信息抓取回来，然后排着队等你检索。

### 爬虫的整体过程

1. 找到需要爬取内容的网页URL
2. 打开该网页的检查页面（谷歌浏览器F12进入开发者模式）
3. 在HTML代码中找到你要提取的数据
4. 写代码对网页进行请求和解析
5. 存储数据

#### chrome打开需要爬取的网站，F12进入开发者模式

!![image-20240101224423171](.\爬虫基础.assets\image-20240101224423171.png)

此时屏幕左侧为你访问的网站内容，屏幕右侧为开发者模式的HTML代码。

鼠标在HTML代码上移动，对应的前端界面部分会被标蓝

我们需要的信息也包含在HTML代码上。

#### 如何从网站请求到这段HTML代码呢

![](E:\pythonProject\PythonLearning\爬虫基础.assets\image-20240101224611973.png)

打开github主页搜索"中山大学" 。

1. 第一步：开发者工具选择Network查看网络状态。
2. 第二步：过滤器选择Doc查看HTML文档相关请求。
3. 第三步：选择合适的请求语句。
4. 查看网络状态发送信息。

注：第三步中几个常用的过滤器解释

| 过滤器名称 | 作用                                         |
| ---------- | -------------------------------------------- |
| **Doc**    | 用于筛选显示与页面文档相关的请求。如HTML页面 |
| JS         | JavaScript动态脚本                           |
| Fetch/XHR  | 网络访问API                                  |
| img        | 图片                                         |

注：第四步中的几个选项解释

- headers：请求头/响应头。HTTP 请求或响应中的头部信息。头部包含请求的方法、响应的状态码、内容类型等。头部通常以键值对的形式出现，每个键值对由冒号分隔，例如 `Content-Type: application/json，代表数据传递模式为json。

![image-20240101224846752](.\爬虫基础.assets\image-20240101224846752.png)

Request URL：请求URL，代表你按下按钮之后，浏览器向这条网址发送了请求。

![](.\爬虫基础.assets\image-20240101224942907.png)

请求方式，代码里要和这里的请求方式保持一致。

| 请求方式 | 描述                                                         |
| -------- | ------------------------------------------------------------ |
| Get      | 最基本的从服务器获取资源的方式，可以将查询字符添加到URL的末尾，查询字符用？代表开始，后面为键值对，用&分隔开 |
| Post     | 向服务器提交表单、数据等                                     |

![](.\爬虫基础.assets\image-20240101224916584.png)

状态码，代表此条访问的运行状态。

| 最常见的状态码            | 描述                                                         |
| ------------------------- | ------------------------------------------------------------ |
| 200 OK                    | Every thing is OK                                            |
| 400 Bad Request           | 服务器无法理解客户端的请求，可能是请求语法错误或者缺参数之类。 |
| 401 Unauthorized          | 请求需要身份验证，要提供验证。                               |
| 403 Forbidden             | 服务器理解了请求，但是拒绝执行，一般是访问没有权限访问的资源。 |
| 404 Not Found             | 请求的资源并不存在。                                         |
| 500 Internal Server Error | 服务器内部错误。                                             |
| 300系列                   | 都是资源重定向类的，意味着你需要向另一处发起请求获得。       |
|                           |                                                              |

![image-20240101210146615](.\爬虫基础.assets\image-20240101210146615.png)

这两个比较重要，其中Accept代表能接收的数据类型。

另外还有一个cookie，这个代表身份识别和会话保持。一般我们请求头会带上cookie，响应头会带上set-cookie要求你重新更新cookie，但有的时候不更新也能正常使用，需要根据具体情况做决定。

User-Agent用于标识发起请求的客户端应用程序、浏览器或设备。它包含了客户端的相关信息，如应用程序名称、版本号、操作系统、浏览器类型等。服务器可以使用该字段来识别客户端并提供适配的响应。我们一般要在代码里显示写上这个，让网站更容易相信我们是正常访问。

- Payload：请求体/响应体。HTTP 请求或响应的主体部分，包含实际的数据。在请求中，负载通常用于发送表单数据、JSON 数据等。在响应中，负载通常包含服务器返回的数据或资源。
- Preview：加了渲染之后的可视化展示。
- Response：响应。也是服务器真正发给你的内容。

### 开始爬虫！

#### 依赖

- requests：用来发送和接收网络请求

- bs4：用来解析HTML文本

- lxml：用来将html解析成tree结构（非必选，但方便查找）

  ```shell
  conda install requests
  conda install beautifulsoup4
  conda install lxml
  ```


#### 使用Requests库发送网络请求

1. 注意以下两点：

   - 请求方式一定和前文1.2中的请求方式保持一致，如爬取github使用的是Get方法，那么我们代码里也要使用Get方法。
   - 请求头的构建：前文1.2中Request Header部分表明了一次访问过程中要封装多少请求头，这里面很多都并不重要，比较重要的是：
     - User Agaent：将开发者工具中显示的Request Header中的User Agaent部分复制到代码中
     - Accept：这个部分表明你渴求什么形式的响应内容形式，根据实践选择是否添加。
     - Cookie：身份验证，如果你访问的是需要登陆的界面的话，那此时一般需要设置Cookie，本任务暂不需要，后面有需求再演示。

   代码示例：

   1. 这段代码展示了我们确定好访问的URL、封装好了合适请求头并使用GET方法向该网址发出了请求。

   ```python
   # 检索URL
   url_filter = "https://github.com/search?q=%E4%B8%AD%E5%B1%B1%E5%A4%A7%E5%AD%A6&type=repositories"
   # Json格式封装请求头
   request_headers = {
       "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
       "Accept" : "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
   }
   
   # 显示句柄指定headers使用上面封装好的request_headers
   response = requests.get(url_filter,headers=request_headers)
   ```

   2. 这段代码展示了获取响应状态码以及获取响应文本。

      一般状态码200系列代表成功、300系列代表重定向，你需要根据它给的网址进一步访问、400系列代表你的访问出现了错误，需要进一步排查。

   ```python
   # 检查响应状态 200 代表OK
   response_statusCode = response.status_code
   print(f"状态码为：{response_statusCode}")
   
   # 获取响应文本
   response_text = response.text
   print(response_text)
   ```

    同样地，获取到的响应文本也可以粘贴到clipboard上使用ctrl+f进行搜索，帮助确认是否为你需要的网页信息。

#### 使用bs4库对html文件内容进行解析。

bs4[官方使用文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

bs4常用的操作（只做介绍，更推荐lxml的写法）：

1. 根据标签查找元素

   ```python
   # 查找第一个<h1>标签
   h1_element = soup.find('h1')
   
   # 查找所有<a>标签
   a_elements = soup.find_all('a')
   
   # 也可以指定内容
   element = soup.find('tag_name', text='desired_text')
   # 这里的 'tag_name' 是要查找的 HTML 标签名，'desired_text' 是要匹配的内容。
   ```

2. 提取元素的文本内容

   ```python
   # 提取元素内的文本内容
   text = element.text
   ```

3. 提取元素的属性

   ```python
   # 提取元素的属性值
   attribute_value = element['attribute_name']
   # 这里的element是要提取属性的HTML元素，attribute_name是要提取的属性名。
   ```

4. 遍历子元素

   ```python
   # 遍历所有子元素
   for child in element.children:
       # 处理子元素
   ```

   

#### 配合lxml使用

lxml可以将bs4生成的soup对象转化为tree结构，支持xpath方式查找，我个人推荐这种做法

[lxml官方文档](https://lxml.de/)

首先我们能看到这一堆标签为<div class="Box-sc-g0xbh4-0 hKtuLA">的元素，这里面每一个元素都对应了左侧界面中的一个条目，我们的目的就是把这些信息爬取下来。

![image-20240102151629221](.\爬虫基础.assets\image-20240102151629221.png)

```python
# 查找所有这些条目列表
items_form = tree.xpath('//div[@class="Box-sc-g0xbh4-0 hKtuLA"]')
# 如果路径不对，返回结果应该是空列表[]，可以用长度判断是否正确找到内容
print(f"查找信息长度:{len(items_form)}")

```

之后我要找到我想要爬取的信息，例如我想获取"中山大学健康申报自动化"这个项目名称，那我首先需要在html文档中找到这部分，分析其所属的分层路径。

**要注意可以使用.开始代表从当前节点开始而不是从根节点开始。//代表不考虑level而从子孙层级中寻找，/表示直接相连的直接子level。**

![image-20240102152056383](.\爬虫基础.assets\image-20240102152056383.png)

```python
# 创建CSV文件，将爬取到的内容写道CSV文件中
with open('./data.csv', mode='w', newline='') as file:
    writer = csv.writer(file)
    # 写入表头
    writer.writerow(['User Name', 'Project Name', 'Project Href'])

    # 遍历数据列表，并将每一行写入CSV文件
    for item in items_form:
        # 带.表示从当前节点开始寻找，不带.则视为从根目录开始寻找
        # //表示查找所有的子孙level     /表示只查找直接子level
        user_name = item.xpath('string(.//div/div/h3//text())')
        project_name = item.xpath('string(.//div/div/div[@class="Box-sc-g0xbh4-0 LjnbQ"])')
        project_href = "https://github.com/" + item.xpath('string(.//div/div/h3//a/@href)')
        print(user_name,project_name,project_href)

        # 将数据写入CSV文件
        writer.writerow([user_name, project_name, project_href])

```

//text()可以直接获取该标签下所有的文本信息，但可能是列表的形式，可以用string进行调整。

此外@href代表获取其超链接，有些超链接形式为/ABC/ABCD，代表为相对路径，可以考虑将前缀的路径拼接上。

到这里这一页的内容就读取完毕了，那么怎么到下一页呢？

![image-20240102152954654](.\爬虫基础.assets\image-20240102152954654.png)

一般页面都会有类似下一页的按钮，我们可以找到对应的连接，对这个连接再次发起访问并重复上述步骤

```python
# Waht about next page
element_page_bottom = tree.xpath('//div[@class="Box-sc-g0xbh4-0 gukfho TablePaginationSteps"]')[0]
href_next_page = None
print(len(element_page_bottom))

# 存在下一页
if(len(element_page_bottom)>0 and element_page_bottom != None):
    # 获取href值用@href
    href_next_page = element_page_bottom.xpath('.//a[@rel="next"]/@href')[0]
    print(f"下一页地址:{href_next_page}")
```

OK，至此你已经get到了基本的爬虫。

但是有一个问题，这种方式面对静态资源加载很合适，但是如果页面是动态渲染的呢？这意味着HTML上并不会一次性列出我们需要的内容，它会在浏览器端进行实时更新。面对这种情况requests的方式就非常尴尬了，后面会介绍另一种模拟人类在网站上进行点击操作的爬虫方式——selenium。