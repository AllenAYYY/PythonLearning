<center><h1 style="font-size: 24px;">爬虫基础</h1></center>

[TOC]



## 在开始之前

在开始正式学习之前，我们先讲一下爬虫是什么以及会涉及到的前端、网络基础知识。

### 什么是爬虫

简单来讲，它的基本操作就是模拟人的行为去各个网站溜达，点点按钮，查查数据，把看到的信息返还回来。

例如我们熟悉的**百度一下**就是应用了这种爬虫技术，每天发放出无数个爬虫到各个网站，把他们的信息抓取回来，然后排着队等你检索。

### 爬虫的整体过程

1. 找到需要爬取内容的网页URL
2. 打开该网页的检查页面（谷歌浏览器F12进入开发者模式）
3. 在HTML代码中找到你要提取的数据
4. 写代码对网页进行请求和解析
5. 存储数据

#### chrome打开需要爬取的网站，F12进入开发者模式

!![image-20240101224423171](.\爬虫基础.assets\image-20240101224423171.png)

此时屏幕左侧为你访问的网站内容，屏幕右侧为开发者模式的HTML代码。

鼠标在HTML代码上移动，对应的前端界面部分会被标蓝

我们需要的信息也包含在HTML代码上。

#### 如何从网站请求到这段HTML代码呢

![](E:\pythonProject\PythonLearning\爬虫基础.assets\image-20240101224611973.png)

打开github主页搜索"中山大学" 。

1. 第一步：开发者工具选择Network查看网络状态。
2. 第二步：过滤器选择Doc查看HTML文档相关请求。
3. 第三步：选择合适的请求语句。
4. 查看网络状态发送信息。

注：第三步中几个常用的过滤器解释

| 过滤器名称 | 作用                                         |
| ---------- | -------------------------------------------- |
| **Doc**    | 用于筛选显示与页面文档相关的请求。如HTML页面 |
| JS         | JavaScript动态脚本                           |
| Fetch/XHR  | 网络访问API                                  |
| img        | 图片                                         |

注：第四步中的几个选项解释

- headers：请求头/响应头。HTTP 请求或响应中的头部信息。头部包含请求的方法、响应的状态码、内容类型等。头部通常以键值对的形式出现，每个键值对由冒号分隔，例如 `Content-Type: application/json，代表数据传递模式为json。

![image-20240101224846752](.\爬虫基础.assets\image-20240101224846752.png)

Request URL：请求URL，代表你按下按钮之后，浏览器向这条网址发送了请求。

![](.\爬虫基础.assets\image-20240101224942907.png)

请求方式，代码里要和这里的请求方式保持一致。

| 请求方式 | 描述                                                         |
| -------- | ------------------------------------------------------------ |
| Get      | 最基本的从服务器获取资源的方式，可以将查询字符添加到URL的末尾，查询字符用？代表开始，后面为键值对，用&分隔开 |
| Post     | 向服务器提交表单、数据等                                     |

![](.\爬虫基础.assets\image-20240101224916584.png)

状态码，代表此条访问的运行状态。

| 最常见的状态码            | 描述                                                         |
| ------------------------- | ------------------------------------------------------------ |
| 200 OK                    | Every thing is OK                                            |
| 400 Bad Request           | 服务器无法理解客户端的请求，可能是请求语法错误或者缺参数之类。 |
| 401 Unauthorized          | 请求需要身份验证，要提供验证。                               |
| 403 Forbidden             | 服务器理解了请求，但是拒绝执行，一般是访问没有权限访问的资源。 |
| 404 Not Found             | 请求的资源并不存在。                                         |
| 500 Internal Server Error | 服务器内部错误。                                             |
| 300系列                   | 都是资源重定向类的，意味着你需要向另一处发起请求获得。       |
|                           |                                                              |

![image-20240101210146615](.\爬虫基础.assets\image-20240101210146615.png)

这两个比较重要，其中Accept代表能接收的数据类型。

另外还有一个cookie，这个代表身份识别和会话保持。一般我们请求头会带上cookie，响应头会带上set-cookie要求你重新更新cookie，但有的时候不更新也能正常使用，需要根据具体情况做决定。

User-Agent用于标识发起请求的客户端应用程序、浏览器或设备。它包含了客户端的相关信息，如应用程序名称、版本号、操作系统、浏览器类型等。服务器可以使用该字段来识别客户端并提供适配的响应。我们一般要在代码里显示写上这个，让网站更容易相信我们是正常访问。

- Payload：请求体/响应体。HTTP 请求或响应的主体部分，包含实际的数据。在请求中，负载通常用于发送表单数据、JSON 数据等。在响应中，负载通常包含服务器返回的数据或资源。
- Preview：加了渲染之后的可视化展示。
- Response：响应。也是服务器真正发给你的内容。

### 开始爬虫！

#### 依赖

- requests：用来发送和接收网络请求

- bs4：用来解析HTML文本

- lxml：用来将html解析成tree结构（非必选，但方便查找）

  ```shell
  conda install requests
  conda install beautifulsoup4
  conda install lxml
  ```


#### 使用Requests库发送网络请求

1. 注意以下两点：

   - 请求方式一定和前文1.2中的请求方式保持一致，如爬取github使用的是Get方法，那么我们代码里也要使用Get方法。
   - 请求头的构建：前文1.2中Request Header部分表明了一次访问过程中要封装多少请求头，这里面很多都并不重要，比较重要的是：
     - User Agaent：将开发者工具中显示的Request Header中的User Agaent部分复制到代码中
     - Accept：这个部分表明你渴求什么形式的响应内容形式，根据实践选择是否添加。
     - Cookie：身份验证，如果你访问的是需要登陆的界面的话，那此时一般需要设置Cookie，本任务暂不需要，后面有需求再演示。

   代码示例：

   1. 这段代码展示了我们确定好访问的URL、封装好了合适请求头并使用GET方法向该网址发出了请求。

   ```python
   # 检索URL
   url_filter = "https://github.com/search?q=%E4%B8%AD%E5%B1%B1%E5%A4%A7%E5%AD%A6&type=repositories"
   # Json格式封装请求头
   request_headers = {
       "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
       "Accept" : "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
   }
   
   # 显示句柄指定headers使用上面封装好的request_headers
   response = requests.get(url_filter,headers=request_headers)
   ```

   2. 这段代码展示了获取响应状态码以及获取响应文本。

      一般状态码200系列代表成功、300系列代表重定向，你需要根据它给的网址进一步访问、400系列代表你的访问出现了错误，需要进一步排查。

   ```python
   # 检查响应状态 200 代表OK
   response_statusCode = response.status_code
   print(f"状态码为：{response_statusCode}")
   
   # 获取响应文本
   response_text = response.text
   print(response_text)
   ```

    同样地，获取到的响应文本也可以粘贴到clipboard上使用ctrl+f进行搜索，帮助确认是否为你需要的网页信息。

#### 使用bs4库对html文件内容进行解析。

bs4[官方使用文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

bs4常用的操作（只做介绍，更推荐lxml的写法）：

1. 根据标签查找元素

   ```python
   # 查找第一个<h1>标签
   h1_element = soup.find('h1')
   
   # 查找所有<a>标签
   a_elements = soup.find_all('a')
   
   # 也可以指定内容
   element = soup.find('tag_name', text='desired_text')
   # 这里的 'tag_name' 是要查找的 HTML 标签名，'desired_text' 是要匹配的内容。
   ```

2. 提取元素的文本内容

   ```python
   # 提取元素内的文本内容
   text = element.text
   ```

3. 提取元素的属性

   ```python
   # 提取元素的属性值
   attribute_value = element['attribute_name']
   # 这里的element是要提取属性的HTML元素，attribute_name是要提取的属性名。
   ```

4. 遍历子元素

   ```python
   # 遍历所有子元素
   for child in element.children:
       # 处理子元素
   ```

   

#### 配合lxml使用

lxml可以将bs4生成的soup对象转化为tree结构，支持xpath方式查找，我个人推荐这种做法

[lxml官方文档](https://lxml.de/)

首先我们能看到这一堆标签为<div class="Box-sc-g0xbh4-0 hKtuLA">的元素，这里面每一个元素都对应了左侧界面中的一个条目，我们的目的就是把这些信息爬取下来。

![image-20240102151629221](.\爬虫基础.assets\image-20240102151629221.png)

```python
# 查找所有这些条目列表
items_form = tree.xpath('//div[@class="Box-sc-g0xbh4-0 hKtuLA"]')
# 如果路径不对，返回结果应该是空列表[]，可以用长度判断是否正确找到内容
print(f"查找信息长度:{len(items_form)}")

```

之后我要找到我想要爬取的信息，例如我想获取"中山大学健康申报自动化"这个项目名称，那我首先需要在html文档中找到这部分，分析其所属的分层路径。

**要注意可以使用.开始代表从当前节点开始而不是从根节点开始。//代表不考虑level而从子孙层级中寻找，/表示直接相连的直接子level。**

![image-20240102152056383](.\爬虫基础.assets\image-20240102152056383.png)

```python
# 创建CSV文件，将爬取到的内容写道CSV文件中
with open('./data.csv', mode='w', newline='') as file:
    writer = csv.writer(file)
    # 写入表头
    writer.writerow(['User Name', 'Project Name', 'Project Href'])

    # 遍历数据列表，并将每一行写入CSV文件
    for item in items_form:
        # 带.表示从当前节点开始寻找，不带.则视为从根目录开始寻找
        # //表示查找所有的子孙level     /表示只查找直接子level
        user_name = item.xpath('string(.//div/div/h3//text())')
        project_name = item.xpath('string(.//div/div/div[@class="Box-sc-g0xbh4-0 LjnbQ"])')
        project_href = "https://github.com/" + item.xpath('string(.//div/div/h3//a/@href)')
        print(user_name,project_name,project_href)

        # 将数据写入CSV文件
        writer.writerow([user_name, project_name, project_href])

```

//text()可以直接获取该标签下所有的文本信息，但可能是列表的形式，可以用string进行调整。

此外@href代表获取其超链接，有些超链接形式为/ABC/ABCD，代表为相对路径，可以考虑将前缀的路径拼接上。

到这里这一页的内容就读取完毕了，那么怎么到下一页呢？

![image-20240102152954654](.\爬虫基础.assets\image-20240102152954654.png)

一般页面都会有类似下一页的按钮，我们可以找到对应的连接，对这个连接再次发起访问并重复上述步骤

```python
# Waht about next page
element_page_bottom = tree.xpath('//div[@class="Box-sc-g0xbh4-0 gukfho TablePaginationSteps"]')[0]
href_next_page = None
print(len(element_page_bottom))

# 存在下一页
if(len(element_page_bottom)>0 and element_page_bottom != None):
    # 获取href值用@href
    href_next_page = element_page_bottom.xpath('.//a[@rel="next"]/@href')[0]
    print(f"下一页地址:{href_next_page}")
```

OK，至此你已经get到了基本的爬虫。

但是有一个问题，这种方式面对静态资源加载很合适，但是如果页面是动态渲染的呢？这意味着HTML上并不会一次性列出我们需要的内容，它会在浏览器端进行实时更新。面对这种情况requests的方式就非常尴尬了，后面会介绍另一种模拟人类在网站上进行点击操作的爬虫方式——selenium。



## 知识点补充

### 正则表达式（regex）

#### 什么是正则表达式

正则表达式是一种被用于从文本中检索某些符合特定模式的文本

正则表达式可以被用来替换字符串中的文本、验证表单、基于模式匹配从一个字符串中提取字符串等等。

#### 元字符

元字符是正则表达式的基本组成元素，元字符在这里跟它通常表达的意思不太一样，需要以特殊的方式去解释，有些元字符写在方括号里有特殊含义。

| 元字符 | 描述                                       |
| ------ | ------------------------------------------ |
| .      | 匹配任意字符                               |
| []     | 字符类，匹配方框中包含的任意字符           |
| [^]    | 否定字符，匹配方框中不包含的任意字符       |
| *      | 匹配前面的子表达式零次或多次               |
| +      | 匹配前面的子表达式一次或多次               |
| ?      | 匹配前面的子表达式零次或一次               |
| {n,m}  | 花括号，匹配前面字符至少n次，但是不超过m次 |
| (xyz)  | 按照顺序匹配字符xyz                        |
| \|     | 匹配符号之前的字符或后面的字符             |
| \      | 转义字符，允许匹配保留字符                 |
| ^      | 匹配行的开始                               |
| $      | 匹配行的结束                               |
| \d     | 匹配任意一个数字(0-9)                      |
| \D     | 匹配任意一个非数字字符                     |
| \S     | 匹配任意一个非空白字符                     |

#### Re库提供的几个正则匹配函数

1. re.match(pattern,string)
   - 功能：从字符串的开头开始尝试匹配，之返回第一个匹配的结果
   - 返回值：如果匹配成功，返回第一个匹配对象；返回失败则返回None
2. re.search(pattern,string)
   - 功能：在字符串中搜索第一个匹配模式的位置，只返回第一个匹配的结果
   - 返回值和上面一样
3. re.findall(pattern,string)
   - 功能：在字符串中找到所有匹配模式的位置，返回一个包含所有匹配结果的列表
   - 返回值：一个列表，每个元素都是一个匹配到的结果字符串
4. re.finditer(pattern,string)
   - 功能：在字符串中找到所有匹配模式的位置，返回一个迭代器，没一个迭代器都是一个match对象。
5. re.split(pattern,string)
   - 功能：根据正则表达式模式拆分字符串
   - 返回值：返回一个列表，其中包含拆分后的字符串

## Scrapy学习

### 什么是Scrapy

Scrapy是一种爬虫框架，和我们之前讲的爬虫方式不同点在于它规定了一套流程，在这个流程里需要做什么都是规定好的，包括数据持久化。

![image-20240111163308501](.\爬虫基础.assets\image-20240111163308501.png)

Scrapy将数据爬取的操作更加细致的划分成了一些模块，都围绕着Scrapy Engine引擎来操作，执行流程就是最开始引擎将某个网站信息发给scheduler调度器调用，Downloader就是下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，Spiders爬取操作就是引擎在Downloader中得到的Response的内容，也就是网页的源码，然后自己操作获取一些想要的数据，在获取时兵分两路：1.如果在爬取过程中被反爬或者请求失败后将请求返回给调度器重新执行流程；2.如果成功了就把拿到的数据结果传输给Item Pipeline，管道的作用主要是将数据打包，存储。大致流程分析就是这样的


### 使用过程

1. 安装框架

```python
conda install scrapy
```

2. 生成爬虫项目

```python
scrapy genspider 文件名称（自定义） 域名（github.com）
```

3. 项目架构

![image-20240111164421422](.\爬虫基础.assets\image-20240111164421422.png)

- spiders下面的py文件即是我们刚刚指定的文件名，也是我们主要写提取数据代码的地方

- items.py文件主要用来声明爬取字段

- pipelines.py主要用来做数据持久化

- settings.py主要用来进行设置，我们一般要把headers头放宽，以及协议遵守修改为False

  ```python
  # 是否遵从协议
  ROBOTSTXT_OBEY = False
  
  # headers头
  DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36',
  }
  
  # 定义管道执行顺序，多个管道执行顺序时会使用
  ITEM_PIPELINES = {
      "myCrawel.pipelines.MycrawelPipeline": 300,
  }
  
  REFERER_POLICY = 'no-referrer'
  
  LOG_LEVEL = "WARNING"
  ```

### 代码流程

1. spiders下的py文件

![image-20240111164949384](.\爬虫基础.assets\image-20240111164949384.png)

主要修改spiders下面的py文件里的parse函数，这里面定义了你处理数据的方式。

2. items.py文件

![image-20240111165733839](.\爬虫基础.assets\image-20240111165733839.png)

这里面是你定义的爬取的字段，比如你这里面定义了name字段，你就可以使用item['name']来表示数据。

3. pipelines文件

这里面是你想做的数据持久化方案，比如规定什么数据保留、什么数据舍弃，存放到数据库里还是CSV文件里。

注意这里面使用到的item['name']和items里声明的要对应上，不然会报错

![image-20240111170032090](.\爬虫基础.assets\image-20240111170032090.png)

4. start文件

这个文件不是项目自动生成，需要自己手动添加，作用是直接运行start文件即可模拟命令行输入命令执行文件，方便操作，不写也没关系

![image-20240111170215079](.\爬虫基础.assets\image-20240111170215079.png)

## Seleinum

Seleinum模拟人点击网页的行为，模拟浏览器行为进行爬取，常用来做自动化测试和爬虫。

它最大的好处就是能做到所见即可爬，对于一些JavaScript动态渲染的页面来说，这种抓取方式非常有效。

### 准备工作

在一切开始之前，你要安装好你的环境。

你需要根据你的chrome浏览器版本安装对应的ChromeDriver驱动，另外python也需要安装好Selenium库。具体安装方法可参考：[CSDN博客](https://blog.csdn.net/qq_48736958/article/details/115179198)

### Let's Go

首先我们明确一点，所有的爬虫方式都是大同小异、殊途同归的。

无外乎都是找到自己想要的网站网址，对这个网址发送请求，打开浏览器的开发者工具(F12)，从HTML中找到自己需要的元素，对这个元素保存或是其他操作。

#### 登录

我们打开[考试宝网站](https://www.zaixiankaoshi.com/login/)

![image-20240111225507119](.\爬虫基础.assets\image-20240111225507119.png)

在开发者界面中发现输入账户的输入框对应的HTML为<input>标签，这种input标签一半就是用来提交数据的

我们也可以看到立即登录这个按钮对应的HTML为<button>标签，这种button对应的就是按钮

![image-20240111225709158](.\爬虫基础.assets\image-20240111225709158.png)

那我们的思路就很明确了，在登陆环节里面我们要干的事情就是

1. 找到<input>标签对应的输入框，将我们的用户名密码填充进去
2. 找到<button>标签，点击这个button，将信息提交到服务器

![image-20240111231906332](.\爬虫基础.assets\image-20240111231906332.png)

我个人推荐使用XPATH的方式进行查找，因为chrome开发者工具能直接复制元素的XPATH，非常方便。

#### 开始爬取题目

你可以看到和之前我们学习的并没有什么区别，都是通过Xpath找到相应的元素

![image-20240111233403102](.\爬虫基础.assets\image-20240111233403102.png)

#### seleinum常用的函数介绍

| 函数                                  | 描述                                         |
| ------------------------------------- | -------------------------------------------- |
| browser = webdriver.Chrome()          | 浏览器对象初始化，后面的Chrome对应你的浏览器 |
| browser.get("https://www.taobao.com") | 通过URL访问对应的网站                        |
|                                       |                                              |
|                                       |                                              |
|                                       |                                              |

1. 初始化浏览器对象

   - 函数：driver = webdriver.Chrome()
   - 描述：浏览器对象初始化，后面的Chrome对应你的浏览器。注意等号左边的driver是自己起的名字，后面的函数都要在这个对象的下面起作用。

2. 访问网站

   - 函数：driver.get("https://www.taobao.com")
   - 描述：通过URL访问对应的网站

3. 查找单个元素

   - 函数写法（比较新的版本）：driver.find_element(By.方法名，查找的字符串)

   - 例子

     ![image-20240112110855741](.\爬虫基础.assets\image-20240112110855741.png)

     常见的几种方法说明

     | 方法名     | 描述                                     |
     | ---------- | ---------------------------------------- |
     | XPATH      | 个人最推荐，能直接从浏览器复制粘贴       |
     | ID         | 通过ID查找                               |
     | CLASS_NAME | 有的标签会设定classname，通过这个查找    |
     | TAG_NAME   | 有的标签会设定tag_name，通过这个查找     |
     | LINK_TEXT  | 通过链接文本来查找，有特殊符号时就会失败 |

4. 查找多个元素

   - 函数写法：driver.find_element**s**,其他的和查找单个元素一样。
   - 与查找单个元素的driver.find_element的区别：
     - find_element只返回一个元素，当匹配到多个元素时，它只返回第一个匹配的元素。如果没有匹配的元素，它将抛出异常。
     - driver.find_element**s**返回所有匹配的元素，将会以列表的形式返回。如果没有找到匹配的元素，它将返回一个空列表。

5. 动作链

   - 有的时候我们只靠一个动作就达成我们的目标，这个时候就需要动作链将多个动作组合起来。例子：有switchbox这种选择开关的时候，把鼠标从一个位置移动到另一个位置并点击。

   - 动作链形式：

     ```python
     from selenium import webdriver
     from selenium.webdriver.common.action_chains import ActionChains
     
     driver = webdriver.Chrome()
     driver.get("https://www.example.com")
     
     source_element = driver.find_element_by_id("sourceElement")
     target_element = driver.find_element_by_id("targetElement")
     
     # 声明动作链
     actions = ActionChains(driver)
     # 声明要做的动作，根据实际需求更改
     actions.drag_and_drop(source_element, target_element)
     # perform会执行前面动作链规定的一系列动作。
     actions.perform()
     ```

   - 常用的动作：

     ```python
     # 拖拽鼠标 move_to_element(元素)
     from selenium import webdriver
     from selenium.webdriver.common.action_chains import ActionChains
     
     driver = webdriver.Chrome()
     driver.get("https://www.example.com")
     
     element = driver.find_element_by_id("myElement")
     
     actions = ActionChains(driver)
     actions.move_to_element(element)
     actions.perform()
     
     
     # 鼠标点击元素 click
     from selenium import webdriver
     from selenium.webdriver.common.action_chains import ActionChains
     
     driver = webdriver.Chrome()
     driver.get("https://www.example.com")
     
     element = driver.find_element_by_id("myElement")
     
     actions = ActionChains(driver)
     actions.click(element)
     actions.perform()
     
     # 鼠标拖放操作,从一个位置拖拽到另一个位置  drag_and_drop(source,target) 
     from selenium import webdriver
     from selenium.webdriver.common.action_chains import ActionChains
     
     driver = webdriver.Chrome()
     driver.get("https://www.example.com")
     
     source_element = driver.find_element_by_id("sourceElement")
     target_element = driver.find_element_by_id("targetElement")
     
     actions = ActionChains(driver)
     actions.drag_and_drop(source_element, target_element)
     actions.perform()
     
     # 键盘操作模拟键盘按键 send_keys(keys)
     from selenium import webdriver
     from selenium.webdriver.common.action_chains import ActionChains
     from selenium.webdriver.common.keys import Keys
     
     driver = webdriver.Chrome()
     driver.get("https://www.example.com")
     
     input_element = driver.find_element_by_id("myInput")
     
     actions = ActionChains(driver)
     actions.send_keys(input_element, "Hello World!")
     actions.send_keys(Keys.ENTER)
     actions.perform()
     
     
     ```

     


